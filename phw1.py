# -*- coding: utf-8 -*-
"""PW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12nkbIYdFZeUoQMa0LyYP-UXwT7QRwLHG
"""

# I use Google Colab as programming tool. 
# And the library below is intended to load the csv file on Google Drive. 
# from typing import Sized
# from google.colab import drive
# drive.mount('/content/gdrive')

# Essential Libraries for Machine Learning 
import pandas as pd
import numpy as np

# Scalers and encoders for data preprocessing 
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, \
                                  RobustScaler, OneHotEncoder, LabelEncoder

# machine learning algorithms 
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# Module for cross validation and module for dividing data into training data and test data 
from sklearn.model_selection import GridSearchCV, train_test_split
# Module for expressing model accuracy 
from sklearn.metrics import accuracy_score

# A function that takes as input a dataset, a list of scalers, and a list of encoders 
# Returns dataframe list to which all of them are applied. 
def ScalerAndEncoder(df, scalers=None, encoders=None, scaled_col=None, encoded_col=None):
  # default parameters
  if scalers is None:
    scalers=[StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler]
  if encoders is None:
    encoders=[OneHotEncoder, LabelEncoder]

  # The elements of the array are arrays and are stored in the following order: 
  # data frame, applied scaler name, and applied encoder. 
  df = pd.DataFrame.copy(df)
  scaledSet = [[df, 'Not scaled']]
  scaledAndEncodedSet = []

  # Apply each scaling to the dataset
  if scaled_col is not None:
    for scaler in scalers:
      temp_df = pd.DataFrame.copy(df)
      scalerInstance = scaler()
      temp_df[scaled_col] = scalerInstance.fit_transform(temp_df[scaled_col])
      scaledSet.append([pd.DataFrame(temp_df), scaler.__name__])

  # Apply each encoder to the scaled data sets above. 
  for scaled in scaledSet:
    scaledAndEncodedSet.append([scaled[0], scaled[1], 'Not encoded'])
    if encoded_col is not None:
      for encoder in encoders:
        temp_df = pd.DataFrame.copy(scaled[0])
        encoderInstance = encoder()
        temp_df[encoded_col] = encoderInstance.fit_transform(temp_df[encoded_col])
        scaledAndEncodedSet.append([pd.DataFrame(temp_df), scaled[1], encoder.__name__])

  return scaledAndEncodedSet

# It takes data and model-specific parameters as inputs, creates 
# DecisionTreeClassifier(gini, entropy), LogisticRegression, and SVM models, 
# and returns the model with the highest accuracy, its parameters, and accuracy. 
# One can also choose how many times to divide the data for cross-validation. 
def TestModel(data, label, params_dtc=None, params_lr=None, params_svc=None, cv=None):
  # default parameters
  if params_dtc is None:
    params_dtc = {'max_depth': list(np.arange(2, 26)),'min_samples_split': list(np.arange(2, 21))}
  if params_lr is None:
    params_lr = {'C': list(np.arange(1, 100, 2)), 'solver': ['lbfgs', 'sag', 'saga']}
  if params_svc is None:
    params_svc = {'kernel': ['linear', 'rbf', 'sigmoid'],'C': list(np.arange(0.01, 10, 0.5)), 'gamma':list(np.arange(0.01, 5, 0.2))}
  if cv is None:
    cv = 3
  # Divide the dataset into training and testing. 
  X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2)

  # Create instances of ML models. 
  dtc_gini = DecisionTreeClassifier(criterion='gini', random_state=0)
  dtc_ent = DecisionTreeClassifier(criterion='entropy', random_state=0)
  lr = LogisticRegression(random_state=0)
  svc = SVC(random_state=0)

  # Create a GridSearchCV instance with each model instance. 
  grid_dtc_gini = GridSearchCV(dtc_gini, param_grid=params_dtc, cv=cv, refit=True)
  grid_dtc_ent = GridSearchCV(dtc_ent, param_grid=params_dtc, cv=cv, refit=True)
  grid_lr = GridSearchCV(lr, param_grid=params_lr, cv=cv, refit=True)
  grid_svc = GridSearchCV(svc, param_grid=params_svc, cv=cv, refit=True)

  # The model training is performed by GridSearchCV
  grid_list = [grid_dtc_gini, grid_dtc_ent, grid_lr, grid_svc]
  grid_combi_list = []
  accuracy_list = []
  for grid in grid_list:
    grid.fit(X_train, y_train)
    em = grid.best_estimator_
    grid_combi_list.append(em)
    pred = em.predict(X_test)
    accuracy_list.append(accuracy_score(y_test, pred))

  best_accuracy = max(accuracy_list)
  best_model = grid_combi_list[accuracy_list.index(best_accuracy)]

  return best_model, best_accuracy

# For each scaling and encoded dataset, we find the model with the highest accuracy, 
# and compare them again to find the best performing combination. 
def FindBestCombi(df, data_col=None, label_col=None, scalers=None, encoders=None,
                  scaled_col=None, encoded_col=None, params_dtc=None, params_lr=None, params_svc=None, cv=None):
  # list of scaling and encoded dataset
  combi_list = ScalerAndEncoder(df, scalers=scalers, encoders=encoders, scaled_col=scaled_col, encoded_col=encoded_col)

  # For each scaling and encoded dataset, find best model
  for scaledAndEncoded in combi_list:
    X = scaledAndEncoded[0][data_col]
    y = scaledAndEncoded[0][label_col]
    semi_best_model, semi_best_accuracy = TestModel(X,y,params_dtc, params_lr, params_svc, cv)
    scaledAndEncoded.append(semi_best_model)
    scaledAndEncoded.append(semi_best_accuracy)

  # get best combination
  combi_accuracy_list = [i[-1] for i in combi_list]
  best_combi = combi_list[combi_accuracy_list.index(max(combi_accuracy_list))]

  # print result
  print('='*100)
  print('Best Combination')
  print('')
  print('Scaler :', best_combi[1])
  print('Encoder :', best_combi[2])
  print('Model and Parameters :', best_combi[3])
  print('Accuracy :', best_combi[4])
  print('='*100)

  best_combi_model = best_combi[3]
  return best_combi_model

# Load dataset as pandas dataframe
# df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/breast-cancer-wisconsin.csv')
df = pd.read_csv('breast-cancer-wisconsin.csv')

# Define predictor variable and target.
col_name = ['Sample code number','Clump Thickness'
,'Uniformity of Cell Size','Uniformity of Cell Shape'
,'Marginal Adhesion','Single Epithelial Cell Sized'
,'Bare Nuclei','Bland Chromatin','Normal Nucleoli'
,'Mitoses','Class']

col_data = ['Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape'
,'Marginal Adhesion','Single Epithelial Cell Sized','Bare Nuclei'
,'Bland Chromatin','Normal Nucleoli','Mitoses']

col_label = 'Class'

# add col_name in dataframe
data = df.values.tolist()
df = pd.DataFrame(data, columns=col_name)
df.drop(['Sample code number'], axis=1, inplace=True)

# Drop missing data point
df['Bare Nuclei'] = df['Bare Nuclei'].replace('?',np.NaN)
df.dropna(inplace=True)
df=df.astype({'Bare Nuclei':np.int})

# Find ML model combination that have highest accuracy.
FindBestCombi(df, data_col=col_data, label_col=col_label)