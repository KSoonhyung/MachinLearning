# -*- coding: utf-8 -*-
"""phw2_my.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oXcgroE-090CsyuZyzpDlmezBmcyrvap
"""

# !pip install pyclustering
# !pip install chart_studio
# !pip install cufflinks

# from google.colab import drive
# drive.mount('/content/gdrive')

# Essential Libraries for Machine Learning 
import pandas as pd
import numpy as np

#for visualising
import matplotlib.pyplot as plt
import seaborn as sns
#plotly imports
import plotly as py
from plotly.subplots import make_subplots
import plotly.express as px
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import chart_studio
chart_studio.tools.set_credentials_file(username='username', api_key='api_key')

# Types to distinguish between scaled_col and encoded_col
from pandas.api.types import is_numeric_dtype
from pandas.api.types import is_string_dtype

# Scalers and encoders for data preprocessing 
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, \
                                  RobustScaler, OneHotEncoder, LabelEncoder

# machine learning algorithms 
from sklearn.cluster import KMeans, DBSCAN, SpectralClustering 
from sklearn.mixture import GaussianMixture
from pyclustering.cluster.clarans import clarans
# for getting score of model
from sklearn.metrics import silhouette_score

# cross validation 
from sklearn.model_selection import GridSearchCV

#Principal Component Analysis
from sklearn.decomposition import PCA 

# to get runtime
import time
import datetime

import warnings

warnings.filterwarnings(action='ignore')

# row 생략 없이 출력
pd.set_option('display.max_rows', None)
# col 생략 없이 출력
pd.set_option('display.max_columns', None)

import sys
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(suppress=True)
np.set_printoptions(precision=3)


def cv_silhouette_scorer(estimator, X):
    labels = estimator.fit_predict(X)
    score = silhouette_score(X, labels, metric='euclidean')
    return score


# function for find the best accuracy given datas, params
def find_best(X, scalers, encoders, models, params_dict=None):
    # save the best accuracy each models
    global best_params
    global score
    score = -1
    best_accuracy = {}
    # find the best parameter by using grid search

    for scaler_key, scaler in scalers.items():
        print(f'--------scaler: {scaler_key}--------')
        X_scaled = scaler.fit_transform(X[get_numeric_col(X)])

        for encoder_key, encoder in encoders.items():
            print(f'------encoder: {encoder_key}------')
            X_encoded = X_scaled.copy()
            for str_col in get_string_col(X):
                X_encoded = encoder.fit_transform(X[str_col].to_numpy().reshape(-1, 1))
                X_encoded = np.concatenate((X_scaled, X_encoded.reshape(X_scaled.shape[0], -1)), axis=1)

            for model_key, model in models.items():
                print(f'----model: {model_key}----')
                start_time = time.time()  # for check running time
                ###############################################################################
                # We need to return a dataframe for the resulting output
                df_clustered = X.copy()
                ###############################################################################

                if model_key == "clarans_model":
                    param_list = list(params_dict[model_key].keys())

                    clarans_params = params_dict[model_key]
                    for p1 in clarans_params[param_list[0]]:
                        for p2 in clarans_params[param_list[1]]:
                            for p3 in clarans_params[param_list[2]]:
                                temp_params = {param_list[0]: p1, param_list[1]: p2,
                                               param_list[2]: p3}

                                clarans_model = clarans(data=X_encoded, number_clusters=p1, numlocal=p2, maxneighbor=p3)
                                clarans_model.process()
                                cluster_result = clarans_model.get_clusters()

                                labels = []
                                for i in range(len(cluster_result)):
                                    for j in cluster_result[i]:
                                        labels.insert(j, i)

                                temp_score = silhouette_score(X_encoded, labels, metric='euclidean')

                                if temp_score > score:
                                    score = temp_score
                                    best_params = temp_params
                else:
                    # grid search
                    cv = [(slice(None), slice(None))]
                    grid = GridSearchCV(estimator=model, param_grid=params_dict[model_key], scoring=cv_silhouette_scorer, cv=cv)
                    grid.fit(X_encoded)
                    best_params = grid.best_params_
                    score = grid.best_score_
                    ###############################################################################
                    
                    estimator = grid.best_estimator_
                    if model_key is "em":
                        labels = estimator.predict(X_encoded)
                    else:
                        labels = estimator.labels_
                    ###############################################################################

                df_clustered['Cluster'] = labels
                print(f'params: {best_params}')

                # save the 3 highest accuracy and parameters each models
                save_len = 3
                save_len -= 1
                flag = False
############################################################################################
                target_dict = {'score': score, 'model': model_key, 'scaler': scaler_key,
                               'encoder': encoder_key, 'param': best_params, 'df_clustered': df_clustered }
############################################################################################
                # save accuracy if best_accuracy has less than save_len items
                if model_key not in best_accuracy.keys():
                    best_accuracy[model_key] = []
                if len(best_accuracy[model_key]) <= save_len:
                    best_accuracy[model_key].append(target_dict)
                    best_accuracy[model_key].sort(key=lambda x: x['score'], reverse=True)
                # insert accuracy for descending
                elif best_accuracy[model_key][-1]['score'] < score:
                    for i in range(1, save_len):
                        if best_accuracy[model_key][save_len - 1 - i]['score'] > score:
                            best_accuracy[model_key].insert(save_len - i, target_dict)
                            best_accuracy[model_key].pop()
                            flag = True
                            break
                    if flag is False:
                        best_accuracy[model_key].insert(0, target_dict)
                        best_accuracy[model_key].pop()

                print(f'score: {score}', end='')
                end_time = time.time()  # for check running time
                print(f'   running time: {end_time - start_time}  cur_time: {datetime.datetime.now()}', end='\n\n')

    return best_accuracy


def get_numeric_col(df):
    numeric_col_list = []

    for col_name in df.columns:
        if is_numeric_dtype(df[col_name].dtypes):
            numeric_col_list.append(col_name)

    return numeric_col_list


def get_string_col(df):
    string_col_list = []

    for col_name in df.columns:
        if is_string_dtype(df[col_name].dtypes):
            string_col_list.append(col_name)

    return string_col_list


def outlier_iqr(df):
    numeric_col_list = get_numeric_col(df)

    for col_name in numeric_col_list:
        q1, q3 = np.percentile(df[col_name], [25, 75])

        iqr = q3 - q1

        lower_bound = q1 - (iqr * 1.5)
        upper_bound = q3 + (iqr * 1.5)

        df = df[upper_bound > df[col_name]]
        df = df[df[col_name] > lower_bound]

    return df


def preprocessing():
    df = pd.read_csv('housing.csv')

    ## Preprocessing

    # # check outlier
    # print(df.iloc[:, :].info())

    # # check NAN
    # print(df.isna().sum().sum())        #has 207 null value

    # print(f'df size:{df.shape[0]}')
    #
    # print(f'number of NAN value:')
    # print(df.isna().sum())      # total_bedrooms has 207 null

    df_drop_NAN = df.dropna(axis=0)     # drop NAN
    # print(f'drop NAN: {df_drop_NAN.shape[0]}')

    # print(df_drop_NAN.describe())

    # fig, ax = plt.subplots(ncols=3, nrows=3)  # visualize outlier
    # for i, col_name in enumerate(get_numeric_col(df_drop_NAN)):
    #     sns.boxplot(data=df_drop_NAN[col_name], ax=ax[int(i/3)][i % 3])
    # plt.show()

    df_drop_outlier = outlier_iqr(df_drop_NAN)
    df_drop_outlier.reset_index(drop=True, inplace=True)

    # print(f'drop outlier: {df_drop_outlier.shape[0]}')

    # fig, ax = plt.subplots(ncols=3, nrows=3)
    #
    # for i, col_name in enumerate(get_numeric_col(df_drop_outlier)): # visualize after delete outlier
    #     sns.boxplot(data=df_drop_outlier[col_name], ax=ax[int(i / 3)][i % 3])
    # plt.show()

    return df_drop_outlier


# function for set hyper parameters and run find_best
def train():
    X = preprocessing()
    X = X.sample(1000)
    ##########################################################################
    X_median_house_value = X['median_house_value']
    X = X.drop('median_house_value', axis=1)
    ##########################################################################
    X = X[['median_income', 'total_rooms', 'population']]

    # 1. Scaler : Standard, MinMax, Robust

    standard = StandardScaler()
    minMax = MinMaxScaler()
    robust = RobustScaler()

    # 2. Encoder : Label, One-Hot

    label_encoder = LabelEncoder()
    oneHot_encoder = OneHotEncoder(sparse=False)

    # 3. Model : KMeans, DBSCAN, GaussianMixture(EM), SpectralClustering

    kmeans = KMeans()
    dbscan = DBSCAN()
    em = GaussianMixture()
    spectral = SpectralClustering()

    # save scalers and models and hyper parameters in dictionary

    scalers = {"standard scaler": standard, "minMax scaler": minMax, "robust scaler": robust}

    encoders = {"one-hot encoder": oneHot_encoder, "label encoder": label_encoder}

    models = {"kmeans": kmeans,
              "em": em, 'spectral': spectral}

    # params_dict = {"kmeans": {"n_clusters": range(2, 12), "tol": [1e-6, 1e-4, 1e-2, 1]},
    #                "dbscan": {"eps": [0.2, 0.5, 0.8], "min_samples": [3, 5, 7, 9]},
    #                "em": {"n_components": [1, 2, 3], "tol": [1e-5, 1e-3, 1e-1, 10]},
    #                "spectral": {"n_clusters": range(2, 12), "gamma": [1, 2]},
    #                "clarans_model": {"number_clusters": range(3, 4), "numlocal": [4, 6, 8],
    #                                  "maxneighbor": [2, 4, 6]}
    #                }

    params_dict = {"kmeans": {"n_clusters": range(2, 7), "tol": [1e-6, 1e-4, 1e-2, 1]},
                   "dbscan": {"eps": [0.2, 0.5, 0.8], "min_samples": [3, 5, 7, 9]},
                   "em": {"n_components": [2, 3,4,5,6,7], "tol": [1e-5, 1e-3, 1e-1, 10]},
                   "spectral": {"n_clusters": range(2, 7), "gamma": [1e-4, 1e-2, 0.1, 1, 2, 3]},
                   "clarans_model": {"number_clusters": range(2, 7), "numlocal": [6, 8],
                                     "maxneighbor": [2, 4]}
                   }

    
    ####################################################################################
    result_dict = find_best(X, scalers, encoders, models, params_dict)
    displayResultDict(result_dict, X_median_house_value)

    return result_dict, X_median_house_value
    # ####################################################################################

################################################################################
# To render in colab
def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',
            },
          });
        </script>
        '''))
################################################################################  

# function for display result_dict
def displayResultDict(result_dict, X_median_house_value):
    #Get best combination that has best score
    #It is saved in best_combi_dict in the type of dictionary
    best_combi:dict = {}
    for model_name, result_list in result_dict.items():
        if len(best_combi) is 0:
                best_combi=result_list[0]
        else:
            if best_combi['score'] < result_list[0]['score']:
                best_combi=result_list[0]

    

    #Get scaled&encoded dataframe that was used to ML
    df = best_combi['df_clustered']
    #Get the number of clusters
    n_clusters:int = len(df['Cluster'].value_counts())

    #and concatenate with X_median_house_value
    df['X_median_house_value'] = X_median_house_value
    #Convert X_median_house_value from continuous to categorical
    mhv_array = df['X_median_house_value'].to_numpy()
    max_mhv = np.max(mhv_array)
    min_mhv = np.min(mhv_array)
    interval_mhv = max_mhv - min_mhv
    step_interval_mhv = interval_mhv / n_clusters
    bins = [step_interval_mhv*i-1 for i in range(n_clusters)]
    df['X_median_house_value'] = np.digitize(df['X_median_house_value'], bins)

    # Sampling for simplicity of visualization
    df = df.sample(500)

    #Get the number of dimension of features that was used for clustering
    # '- 2' means drop 'Cluster','X_median_house_value'
    n_dimension:int = len(df.columns) - 2
    #Get columns' names
    col_name_list:list = df.columns.values.tolist()

    print('='*100)
    print('----------Best Combination----------\n')
    print('Scaler :', best_combi['scaler'])
    print('Encoder :', best_combi['encoder'])
    print('Model :', best_combi['model'])
    print('Parameters :', best_combi['param'])
    print('Score :', best_combi['score'])
    print('Number of clusters :', n_clusters)

    #Draw plot according to dinmension
    if n_dimension is 2:
        title = f'Visualizing Clusters with {col_name_list[0]}, {col_name_list[1]}\n\
                Number of Clusters : {n_clusters}'
        fig = px.scatter(df, x=col_name_list[0], y=col_name_list[1], color='Cluster',
                 title = title)
        
    elif n_dimension is 3:
        title = f'Visualizing Clusters with {col_name_list[0]}, {col_name_list[1]}, {col_name_list[2]}\n\
                Number of Clusters : {n_clusters}'
        fig = px.scatter_3d(df, x=col_name_list[0], y=col_name_list[1], z=col_name_list[2], color='Cluster',
                 title = title)

    elif n_dimension is 4:
        title = f'Visualizing Clusters with {col_name_list[0]}, {col_name_list[1]}, {col_name_list[2]}, {col_name_list[3]}\n\
                Number of Clusters : {n_clusters}'
        fig = px.scatter_3d(df, x=col_name_list[1], y=col_name_list[2], z=col_name_list[3], color='Cluster',
                 size=col_name_list[0], title = title)

        
    #########################################################################
    # For colab
    configure_plotly_browser_state()
    init_notebook_mode(connected=False)
    #########################################################################
    # This is needed so we can display plotly plots properly
    # For local environment
    #init_notebook_mode(connected=False)
    #########################################################################

    fig.show()

    print('\n-----Does our clustering mean median_house_value?-----')
    print('Compare our clustering and median_house_value using PCA')

    #PCA with two principal components
    str_col_list = get_string_col(df)
    if len(str_col_list) is not 0:
        for str_col in str_col_list:
            encoder = LabelEncoder()
            X_encoded = encoder.fit_transform(df[str_col].to_numpy().reshape(-1, 1))
            df[str_col] = X_encoded
    pca_2d = PCA(n_components=2)
    PCs_2d = pd.DataFrame(pca_2d.fit_transform(df.drop(['Cluster', 'X_median_house_value'], axis=1)))
    PCs_2d.columns = ['PC1_2d', 'PC2_2d']
    PCs_2d['Cluster'] = df['Cluster'].to_numpy().reshape(-1, 1)
    PCs_2d['X_median_house_value'] = df['X_median_house_value'].to_numpy().reshape(-1, 1)

    # Draw Cluster plot
    # Draw median house value plot
    fig = make_subplots(
                        rows=1, cols=2,
                        subplot_titles=("Clusters plot","Median house value plot"))
    
    fig.add_trace(go.Scatter(x = PCs_2d["PC1_2d"],
                y = PCs_2d["PC2_2d"],
                mode = "markers",
                marker = dict(color = PCs_2d['Cluster']),
                text = None), row=1, col=1)
    
    fig.add_trace(go.Scatter(x = PCs_2d["PC1_2d"],
                y = PCs_2d["PC2_2d"],
                mode = "markers",
                marker = dict(color = PCs_2d['X_median_house_value']),
                text = None), row=1, col=2)
    fig.update_xaxes(title_text="pc1", row=1, col=1)
    fig.update_xaxes(title_text="pc1", row=1, col=2)

    fig.update_yaxes(title_text="pc2", row=1, col=1)
    fig.update_yaxes(title_text="pc2", row=1, col=2)

    fig.update_layout(title_text="Compare Clustering and median_house_value using PCA")

    fig.show()
    

    print('='*100)




if __name__ == "__main__":
    train()